{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353a9825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T11:05:12.665932Z",
     "iopub.status.busy": "2025-02-17T11:05:12.665743Z",
     "iopub.status.idle": "2025-02-17T11:05:13.102351Z",
     "shell.execute_reply": "2025-02-17T11:05:13.101994Z"
    },
    "papermill": {
     "duration": 0.440606,
     "end_time": "2025-02-17T11:05:13.103314",
     "exception": false,
     "start_time": "2025-02-17T11:05:12.662708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Installing the required packages if not already installed\n",
    "packages = ['numpy', 'pandas', 'warnings', 'sqlite3', 'yfinance', 'numba', 'time']\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        %pip install {package}\n",
    "\n",
    "\n",
    "### Start timer\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sqlite3\n",
    "import yfinance as yf\n",
    "import os\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "### Ignoring the warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "### Setting working directory\n",
    "os.chdir('/Users/emilwilliamhansen/Library/Mobile Documents/com~apple~CloudDocs/School/Master Thesis/Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f23c4d3",
   "metadata": {
    "papermill": {
     "duration": 0.002737,
     "end_time": "2025-02-17T11:05:13.111250",
     "exception": false,
     "start_time": "2025-02-17T11:05:13.108513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Reading the data - Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cefdfaf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T11:05:13.115830Z",
     "iopub.status.busy": "2025-02-17T11:05:13.115620Z",
     "iopub.status.idle": "2025-02-17T11:05:13.198689Z",
     "shell.execute_reply": "2025-02-17T11:05:13.198273Z"
    },
    "papermill": {
     "duration": 0.086049,
     "end_time": "2025-02-17T11:05:13.199520",
     "exception": false,
     "start_time": "2025-02-17T11:05:13.113471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>ticker</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Return</th>\n",
       "      <th>Price</th>\n",
       "      <th>Shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO0003069908</td>\n",
       "      <td>AAT</td>\n",
       "      <td>Aust-Agder Trafikkselskap</td>\n",
       "      <td>1980-01-31</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>240.00</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO0003069908</td>\n",
       "      <td>AAT</td>\n",
       "      <td>Aust-Agder Trafikkselskap</td>\n",
       "      <td>1980-02-29</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>240.00</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NO0003069908</td>\n",
       "      <td>AAT</td>\n",
       "      <td>Aust-Agder Trafikkselskap</td>\n",
       "      <td>1980-03-31</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>240.00</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO0003069908</td>\n",
       "      <td>AAT</td>\n",
       "      <td>Aust-Agder Trafikkselskap</td>\n",
       "      <td>1980-04-30</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>250.00</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO0003069908</td>\n",
       "      <td>AAT</td>\n",
       "      <td>Aust-Agder Trafikkselskap</td>\n",
       "      <td>1980-05-31</td>\n",
       "      <td>0.4600</td>\n",
       "      <td>325.00</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14953</th>\n",
       "      <td>US36467X2062</td>\n",
       "      <td>GIG</td>\n",
       "      <td>GAMING INNOVATION</td>\n",
       "      <td>2024-08-31</td>\n",
       "      <td>-0.0334</td>\n",
       "      <td>28.95</td>\n",
       "      <td>127132040.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14954</th>\n",
       "      <td>VGG3175Q1081</td>\n",
       "      <td>EPIC</td>\n",
       "      <td>Epic Gas</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>16.00</td>\n",
       "      <td>106616352.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14955</th>\n",
       "      <td>VGG3175Q1081</td>\n",
       "      <td>EPIC</td>\n",
       "      <td>Epic Gas</td>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>-0.0437</td>\n",
       "      <td>15.30</td>\n",
       "      <td>106616352.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14956</th>\n",
       "      <td>VGG3175Q1081</td>\n",
       "      <td>EPIC</td>\n",
       "      <td>EPIC GAS</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>0.1432</td>\n",
       "      <td>16.30</td>\n",
       "      <td>106616352.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14957</th>\n",
       "      <td>VGG3175Q1081</td>\n",
       "      <td>EPICT</td>\n",
       "      <td>EPIC GAS</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>-0.0798</td>\n",
       "      <td>15.00</td>\n",
       "      <td>106616352.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106912 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ISIN ticker                       Name       Date  Return  \\\n",
       "0      NO0003069908    AAT  Aust-Agder Trafikkselskap 1980-01-31  0.0000   \n",
       "1      NO0003069908    AAT  Aust-Agder Trafikkselskap 1980-02-29  0.0000   \n",
       "2      NO0003069908    AAT  Aust-Agder Trafikkselskap 1980-03-31  0.0000   \n",
       "3      NO0003069908    AAT  Aust-Agder Trafikkselskap 1980-04-30  0.0417   \n",
       "4      NO0003069908    AAT  Aust-Agder Trafikkselskap 1980-05-31  0.4600   \n",
       "...             ...    ...                        ...        ...     ...   \n",
       "14953  US36467X2062    GIG          GAMING INNOVATION 2024-08-31 -0.0334   \n",
       "14954  VGG3175Q1081   EPIC                   Epic Gas 2020-12-31  0.0191   \n",
       "14955  VGG3175Q1081   EPIC                   Epic Gas 2021-01-31 -0.0437   \n",
       "14956  VGG3175Q1081   EPIC                   EPIC GAS 2021-02-28  0.1432   \n",
       "14957  VGG3175Q1081  EPICT                   EPIC GAS 2021-03-31 -0.0798   \n",
       "\n",
       "        Price       Shares  \n",
       "0      240.00       4000.0  \n",
       "1      240.00       4000.0  \n",
       "2      240.00       4000.0  \n",
       "3      250.00       4000.0  \n",
       "4      325.00       4000.0  \n",
       "...       ...          ...  \n",
       "14953   28.95  127132040.0  \n",
       "14954   16.00  106616352.0  \n",
       "14955   15.30  106616352.0  \n",
       "14956   16.30  106616352.0  \n",
       "14957   15.00  106616352.0  \n",
       "\n",
       "[106912 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Reading the datasets\n",
    "monthly_80_20 = (pd.read_csv(\"Data/monthly_stock_returns_ose.csv\",\n",
    "                      sep=';', encoding='latin1')[[\"ISIN\", \"ticker\", \"Last_Sec_Name\", \"Date\", \"MonthlyReturn\", \"LastPrice\", \"NoShares\"]]\n",
    "                      ).rename(columns={\"Last_Sec_Name\": \"Name\", \"MonthlyReturn\": \"Return\", \"LastPrice\": \"Price\", \"NoShares\": \"Shares\"})\n",
    "\n",
    "monthly_20_24 = (pd.read_csv('Data/ose_equity_euronext_data/monthly_ose_stocks_nov_2020_aug_2024.csv',\n",
    "                         sep=',', encoding='latin1')[[\"ISIN\", \"ticker\", \"Name\", \"Date\", \"Return\", \"Price\", \"SharesOutstanding\"]]\n",
    "                         ).rename(columns={\"SharesOutstanding\": \"Shares\"})\n",
    "\n",
    "### Combining the datasets\n",
    "monthly = pd.concat([monthly_80_20, monthly_20_24])\n",
    "#del monthly_80_20, monthly_20_24\n",
    "\n",
    "### Fixing the date format\n",
    "monthly[\"Date\"] = pd.to_datetime(monthly[\"Date\"], format=\"%Y%m%d\")\n",
    "\n",
    "monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a6df45",
   "metadata": {
    "papermill": {
     "duration": 0.001842,
     "end_time": "2025-02-17T11:05:13.203809",
     "exception": false,
     "start_time": "2025-02-17T11:05:13.201967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Reading the data - Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e49367a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T11:05:13.208216Z",
     "iopub.status.busy": "2025-02-17T11:05:13.208046Z",
     "iopub.status.idle": "2025-02-17T11:05:33.245358Z",
     "shell.execute_reply": "2025-02-17T11:05:33.244802Z"
    },
    "papermill": {
     "duration": 20.041877,
     "end_time": "2025-02-17T11:05:33.247383",
     "exception": false,
     "start_time": "2025-02-17T11:05:13.205506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>ticker</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Return</th>\n",
       "      <th>Price</th>\n",
       "      <th>Shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO0003069908</td>\n",
       "      <td>AAT</td>\n",
       "      <td>Aust-Agder Trafikkselskap</td>\n",
       "      <td>1980-01-04</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO0003069908</td>\n",
       "      <td>AAT</td>\n",
       "      <td>Aust-Agder Trafikkselskap</td>\n",
       "      <td>1980-01-08</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NO0003069908</td>\n",
       "      <td>AAT</td>\n",
       "      <td>Aust-Agder Trafikkselskap</td>\n",
       "      <td>1980-01-11</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO0003069908</td>\n",
       "      <td>AAT</td>\n",
       "      <td>Aust-Agder Trafikkselskap</td>\n",
       "      <td>1980-01-15</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO0003069908</td>\n",
       "      <td>AAT</td>\n",
       "      <td>Aust-Agder Trafikkselskap</td>\n",
       "      <td>1980-01-18</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35407819</th>\n",
       "      <td>VGG3175Q1081</td>\n",
       "      <td>EPIC</td>\n",
       "      <td>EPIC GAS</td>\n",
       "      <td>2021-03-04</td>\n",
       "      <td>-0.0338</td>\n",
       "      <td>14.3</td>\n",
       "      <td>106616352.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35407820</th>\n",
       "      <td>VGG3175Q1081</td>\n",
       "      <td>EPIC</td>\n",
       "      <td>EPIC GAS</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>-0.0350</td>\n",
       "      <td>13.8</td>\n",
       "      <td>106616352.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35407821</th>\n",
       "      <td>VGG3175Q1081</td>\n",
       "      <td>EPIC</td>\n",
       "      <td>EPIC GAS</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>-0.0350</td>\n",
       "      <td>13.8</td>\n",
       "      <td>106616352.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35407822</th>\n",
       "      <td>VGG3175Q1081</td>\n",
       "      <td>EPIC</td>\n",
       "      <td>EPIC GAS</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>-0.0350</td>\n",
       "      <td>13.8</td>\n",
       "      <td>106616352.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35407823</th>\n",
       "      <td>VGG3175Q1081</td>\n",
       "      <td>EPICT</td>\n",
       "      <td>EPIC GAS</td>\n",
       "      <td>2021-03-09</td>\n",
       "      <td>0.0870</td>\n",
       "      <td>15.0</td>\n",
       "      <td>106616352.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37291557 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ISIN ticker                       Name       Date  Return  \\\n",
       "0         NO0003069908    AAT  Aust-Agder Trafikkselskap 1980-01-04  0.0000   \n",
       "1         NO0003069908    AAT  Aust-Agder Trafikkselskap 1980-01-08  0.0000   \n",
       "2         NO0003069908    AAT  Aust-Agder Trafikkselskap 1980-01-11  0.0000   \n",
       "3         NO0003069908    AAT  Aust-Agder Trafikkselskap 1980-01-15  0.0000   \n",
       "4         NO0003069908    AAT  Aust-Agder Trafikkselskap 1980-01-18  0.0000   \n",
       "...                ...    ...                        ...        ...     ...   \n",
       "35407819  VGG3175Q1081   EPIC                   EPIC GAS 2021-03-04 -0.0338   \n",
       "35407820  VGG3175Q1081   EPIC                   EPIC GAS 2021-03-05 -0.0350   \n",
       "35407821  VGG3175Q1081   EPIC                   EPIC GAS 2021-03-05 -0.0350   \n",
       "35407822  VGG3175Q1081   EPIC                   EPIC GAS 2021-03-05 -0.0350   \n",
       "35407823  VGG3175Q1081  EPICT                   EPIC GAS 2021-03-09  0.0870   \n",
       "\n",
       "          Price       Shares  \n",
       "0           NaN       4000.0  \n",
       "1           NaN       4000.0  \n",
       "2           NaN       4000.0  \n",
       "3           NaN       4000.0  \n",
       "4           NaN       4000.0  \n",
       "...         ...          ...  \n",
       "35407819   14.3  106616352.0  \n",
       "35407820   13.8  106616352.0  \n",
       "35407821   13.8  106616352.0  \n",
       "35407822   13.8  106616352.0  \n",
       "35407823   15.0  106616352.0  \n",
       "\n",
       "[37291557 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_80_90 = (pd.read_csv(\"Data/daily_stock_returns_ose_csv/daily_stock_returns_ose_1980_1989.csv\",\n",
    "                        sep=';', encoding='latin1')[[\"ISIN\", \"ticker\", \"Last_Sec_Name\", \"Date\", \"Return\", \"ClosePrice\", \"SharesOutstanding\"]]\n",
    "                        ).rename(columns={\"Last_Sec_Name\": \"Name\", \"ClosePrice\": \"Price\", \"SharesOutstanding\": \"Shares\"})\n",
    "\n",
    "daily_90_00 = (pd.read_csv(\"Data/daily_stock_returns_ose_csv/daily_stock_returns_ose_1990_1999.csv\",\n",
    "                        sep=';', encoding='latin1')[[\"ISIN\", \"ticker\", \"Last_Sec_Name\", \"Date\", \"Return\", \"ClosePrice\", \"SharesOutstanding\"]]\n",
    "                        ).rename(columns={\"Last_Sec_Name\": \"Name\", \"ClosePrice\": \"Price\", \"SharesOutstanding\": \"Shares\"})\n",
    "\n",
    "daily_00_10 = (pd.read_csv(\"Data/daily_stock_returns_ose_csv/daily_stock_returns_ose_2000_2009.csv\",\n",
    "                        sep=';', encoding='latin1')[[\"ISIN\", \"ticker\", \"Last_Sec_Name\", \"Date\", \"Return\", \"ClosePrice\", \"SharesOutstanding\"]]\n",
    "                        ).rename(columns={\"Last_Sec_Name\": \"Name\", \"ClosePrice\": \"Price\", \"SharesOutstanding\": \"Shares\"})\n",
    "\n",
    "daily_10_20 = (pd.read_csv(\"Data/daily_stock_returns_ose_csv/daily_stock_returns_ose_2010_2020.csv\",\n",
    "                        sep=';', encoding='latin1')[[\"ISIN\", \"ticker\", \"Last_Sec_Name\", \"Date\", \"Return\", \"ClosePrice\", \"SharesOutstanding\"]]\n",
    "                        ).rename(columns={\"Last_Sec_Name\": \"Name\", \"ClosePrice\": \"Price\", \"SharesOutstanding\": \"Shares\"})\n",
    "\n",
    "daily_20_24 = (pd.read_csv('Data/ose_equity_euronext_data/daily_ose_stocks_nov_2020_aug_2024.csv',\n",
    "                         sep=';', encoding='latin1')[[\"ticker\", \"Name\", \"Date\", \"Return\", \"ClosePrice\", \"SharesOutstanding\"]]\n",
    "                        ).rename(columns={\"ClosePrice\": \"Price\", \"SharesOutstanding\": \"Shares\"})\n",
    "\n",
    "### Fixing the ISIN values for 2020-2024 dataset by extracing the ISIN values from the monthly dataset\n",
    "daily_20_24['ISIN'] = np.nan\n",
    "daily_20_24 = daily_20_24.merge(monthly[[\"ISIN\", \"ticker\"]], on=\"ticker\", how=\"left\").drop(columns='ISIN_x').rename(columns={\"ISIN_y\": \"ISIN\"})\n",
    "\n",
    "### Combining the datasets\n",
    "daily = pd.concat([daily_80_90, daily_90_00, daily_00_10, daily_10_20, daily_20_24])\n",
    "del daily_80_90, daily_90_00, daily_00_10, daily_10_20, daily_20_24\n",
    "\n",
    "\n",
    "### Fixing the date format\n",
    "daily[\"Date\"] = pd.to_datetime(daily[\"Date\"], format=\"%Y%m%d\")\n",
    "\n",
    "daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c547e970",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T11:05:33.254319Z",
     "iopub.status.busy": "2025-02-17T11:05:33.254091Z",
     "iopub.status.idle": "2025-02-17T11:05:41.359886Z",
     "shell.execute_reply": "2025-02-17T11:05:41.358807Z"
    },
    "papermill": {
     "duration": 8.111628,
     "end_time": "2025-02-17T11:05:41.362709",
     "exception": false,
     "start_time": "2025-02-17T11:05:33.251081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Getting the values unique rows for ISIN and ticker\n",
    "isin_ticker = pd.concat([monthly[[\"ISIN\", \"ticker\"]], daily[[\"ISIN\", \"ticker\"]]]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "### Getting the rows where the ISIN values are not unique\n",
    "isin = isin_ticker[isin_ticker.duplicated(subset='ISIN', keep=False)].sort_index().sort_values(by='ISIN')\n",
    "\n",
    "### If a row has a NaN value in the ticker column, then we keep the row with the ticker value\n",
    "isin = isin.dropna(subset=['ticker'])\n",
    "\n",
    "### Dropping the duplicate ISINs, but keeping the last one\n",
    "isin = isin.drop_duplicates(subset='ISIN', keep='last')\n",
    "\n",
    "### Creating a dictionary to map ISIN to ticker\n",
    "isin_dict = isin.set_index('ISIN')['ticker'].to_dict()\n",
    "\n",
    "### Updating the ticker column in the daily and monthly datasets\n",
    "daily['ticker'] = daily['ISIN'].map(isin_dict).fillna(daily['ticker'])\n",
    "monthly['ticker'] = monthly['ISIN'].map(isin_dict).fillna(monthly['ticker'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11100381",
   "metadata": {
    "papermill": {
     "duration": 0.002507,
     "end_time": "2025-02-17T11:05:41.370493",
     "exception": false,
     "start_time": "2025-02-17T11:05:41.367986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Looking at all our tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d6030ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T11:05:41.379699Z",
     "iopub.status.busy": "2025-02-17T11:05:41.379468Z",
     "iopub.status.idle": "2025-02-17T11:05:43.025757Z",
     "shell.execute_reply": "2025-02-17T11:05:43.025211Z"
    },
    "papermill": {
     "duration": 0.766805,
     "end_time": "2025-02-17T11:05:42.141079",
     "exception": false,
     "start_time": "2025-02-17T11:05:41.374274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Lets see if the tickers overlap as expected\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m daily_tickers \u001b[38;5;241m=\u001b[39m \u001b[43mdaily\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mticker\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m monthly_tickers \u001b[38;5;241m=\u001b[39m monthly[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m      5\u001b[0m missing_tickers_daily \u001b[38;5;241m=\u001b[39m [ticker \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m monthly_tickers \u001b[38;5;28;01mif\u001b[39;00m ticker \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m daily_tickers]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/series.py:2407\u001b[0m, in \u001b[0;36mSeries.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;124;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[1;32m   2347\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2405\u001b[0m \u001b[38;5;124;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[1;32m   2406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/base.py:1025\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/algorithms.py:401\u001b[0m, in \u001b[0;36munique\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21munique\u001b[39m(values):\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/algorithms.py:440\u001b[0m, in \u001b[0;36munique_with_mask\u001b[0;34m(values, mask)\u001b[0m\n\u001b[1;32m    438\u001b[0m table \u001b[38;5;241m=\u001b[39m hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Lets see if the tickers overlap as expected\n",
    "daily_tickers = daily['ticker'].unique()\n",
    "monthly_tickers = monthly['ticker'].unique()\n",
    "\n",
    "missing_tickers_daily = [ticker for ticker in monthly_tickers if ticker not in daily_tickers]\n",
    "print(f\"We have {len(missing_tickers_daily)} tickers in monthly that are not in daily\")\n",
    "\n",
    "missing_tickers_monthly = [ticker for ticker in daily_tickers if ticker not in monthly_tickers]\n",
    "print(f\"We have {len(missing_tickers_monthly)} tickers in daily that are not in monthly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2a080",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Looking at only the tickers that are in missing_tickers_monthly\n",
    "daily[daily['ticker'].isin(missing_tickers_monthly)].value_counts(\"Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9617c4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So, these dont even have a full trading month of returns, so no reason to include them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b50cbc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Removing the ticker is in missing_tickers_daily, remove the row\n",
    "daily = daily[~daily['ticker'].isin(missing_tickers_daily)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59c7cd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Getting all the unique tickers from both datasets\n",
    "companies = pd.concat([daily, monthly])[['ticker', 'Name']].drop_duplicates(subset='ticker').sort_values('ticker').reset_index(drop=True)\n",
    "companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00492c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure dates and tickers are in the right order\n",
    "dates = daily['Date'].drop_duplicates().sort_values()\n",
    "tickers = companies['ticker']\n",
    "\n",
    "### Lets create the pivot tables\n",
    "daily_returns = daily.pivot_table(\n",
    "    index='Date',\n",
    "    columns='ticker',\n",
    "    values='Return',\n",
    "    aggfunc='first'\n",
    ").reindex(index=dates, columns=tickers)\n",
    "\n",
    "daily_prices = daily.pivot_table(\n",
    "    index='Date',\n",
    "    columns='ticker',\n",
    "    values='Price',\n",
    "    aggfunc='first'\n",
    ").reindex(index=dates, columns=tickers)\n",
    "\n",
    "daily_shares = daily.pivot_table(\n",
    "    index='Date',\n",
    "    columns='ticker',\n",
    "    values='Shares',\n",
    "    aggfunc='first'\n",
    ").reindex(index=dates, columns=tickers)\n",
    "\n",
    "daily_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93aca3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure dates and tickers are in the right order\n",
    "dates = monthly['Date'].drop_duplicates().sort_values()\n",
    "\n",
    "### Lets turn the long format into wide format\n",
    "monthly_returns = monthly.pivot_table(\n",
    "    index='Date',\n",
    "    columns='ticker',\n",
    "    values='Return',\n",
    "    aggfunc='first'\n",
    ").reindex(index=dates, columns=tickers)\n",
    "\n",
    "monthly_prices = monthly.pivot_table(\n",
    "    index='Date',\n",
    "    columns='ticker',\n",
    "    values='Price',\n",
    "    aggfunc='first'\n",
    ").reindex(index=dates, columns=tickers)\n",
    "\n",
    "monthly_shares = monthly.pivot_table(\n",
    "    index='Date',\n",
    "    columns='ticker',\n",
    "    values='Shares',\n",
    "    aggfunc='first'\n",
    ").reindex(index=dates, columns=tickers)\n",
    "\n",
    "monthly_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be5aea7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Lets download the daily prices of the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885c2e3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Getting the tickers\n",
    "# yf_ticker = [i + \".OL\" for i in daily_prices.columns]\n",
    "\n",
    "# ### Downloading the data\n",
    "# yf_price = yf.download(yf_ticker, start='2020-01-01', end='2020-12-31')['Close']\n",
    "\n",
    "# ### Removing the .OL from the columns\n",
    "# yf_price.columns = [i.replace(\".OL\", \"\") for i in yf_price.columns]\n",
    "\n",
    "# ### Making sure the index is a datetime\n",
    "# yf_price.index = pd.to_datetime(yf_price.index)\n",
    "\n",
    "# ### Saving the downloaded data as a csv\n",
    "# yf_price.to_csv('Data/data_2020_yfinance.csv')\n",
    "\n",
    "### Reading the data\n",
    "yf_price = pd.read_csv('Data/data_2020_yfinance.csv', index_col=0)\n",
    "\n",
    "### Making sure the index is a datetime\n",
    "yf_price.index = pd.to_datetime(yf_price.index)\n",
    "\n",
    "### Getting the returns\n",
    "yf_returns = yf_price.pct_change()\n",
    "\n",
    "### Drop the first row\n",
    "yf_returns = yf_returns.iloc[1:]\n",
    "\n",
    "### Replace the 0 with NaN\n",
    "yf_returns = yf_returns.replace(0, np.nan)\n",
    "\n",
    "yf_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3587ea",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Getting the extra prices from CIQ\n",
    "# ciq_price = pd.read_excel(\"Data/data_2020_CIQ.xlsx\", index_col=0)\n",
    "\n",
    "# ### Making sure the index is a datetime\n",
    "# ciq_price.index = pd.to_datetime(ciq_price.index)\n",
    "\n",
    "# ### Removing -OB from the columns\n",
    "# ciq_price.columns = [i.replace(\"-OB\", \"\") for i in ciq_price.columns]\n",
    "\n",
    "# ### Remove the columns that are only NaN\n",
    "# ciq_price = ciq_price.dropna(axis=1, how='all')\n",
    "\n",
    "# ### Removing 3 columns that are not in the original data\n",
    "# ciq_price = ciq_price.drop(columns=['MELG-OSL', 'SB1NO', 'SOR.1', 'OBSRV', 'PROT', 'REACH', 'SCANA', 'STRO', 'SUBC', 'TGS', 'ULTI', 'ABG', 'AGAS'])\n",
    "\n",
    "# ### Getting the returns\n",
    "# ciq_returns = ciq_price.pct_change()\n",
    "\n",
    "# ### Drop the first row\n",
    "# ciq_returns = ciq_returns.iloc[1:]\n",
    "\n",
    "# ### Replace the 0 with NaN\n",
    "# ciq_returns = ciq_returns.replace(0, np.nan)\n",
    "\n",
    "# ### Reindexing ciq_returns to match yf_returns\n",
    "# ciq_returns = ciq_returns.reindex(index=yf_returns.index, columns=yf_returns.columns)\n",
    "\n",
    "# #ciq_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584cd92",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Checking for NaNs\n",
    "# nanprice = yf_price.isnull().sum().sum()\n",
    "# nanreturns = yf_returns.isnull().sum().sum()\n",
    "# print(f\"We have {nanprice} NaNs in the price dataset\")\n",
    "# print(f\"We have {nanreturns} NaNs in the returns dataset\")\n",
    "\n",
    "# ### Lets use ciq_price to fill in the missing values in yf_price\n",
    "# for tickers in ciq_price.columns:\n",
    "\n",
    "#     ### Filling the missing prices\n",
    "#     for i in range(len(ciq_price)):\n",
    "#         if np.isnan(yf_price.loc[ciq_price.index[i], tickers]):\n",
    "#             yf_price.loc[ciq_price.index[i], tickers] = ciq_price.loc[ciq_price.index[i], tickers]\n",
    "        \n",
    "#     ### Filling the missing returns\n",
    "#     for i in range(len(ciq_returns)):\n",
    "#         if np.isnan(yf_returns.loc[ciq_returns.index[i], tickers]):\n",
    "#             yf_returns.loc[ciq_returns.index[i], tickers] = ciq_returns.loc[ciq_returns.index[i], tickers]\n",
    "\n",
    "# ### Checking NaNs\n",
    "# nanprice2 = yf_price.isnull().sum().sum()\n",
    "# nanreturns2 = yf_returns.isnull().sum().sum()\n",
    "# print(f\"We have {nanprice2} NaNs in the price dataset, so we filled in {nanprice - nanprice2} NaNs\")\n",
    "# print(f\"We have {nanreturns2} NaNs in the returns dataset, so we filled in {nanreturns - nanreturns2} NaNs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a19d1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Getting the monthly data\n",
    "yf_returns_monthly = yf_returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
    "yf_price_monthly = yf_price.resample('M').last()\n",
    "\n",
    "### Replace the 0 with NaN\n",
    "yf_returns_monthly = yf_returns_monthly.replace(0, np.nan)\n",
    "yf_price_monthly = yf_price_monthly.replace(0, np.nan)\n",
    "\n",
    "### Restricting the data to only the ones we need\n",
    "yf_returns = yf_returns.loc['2020-07-01':'2020-11-30']\n",
    "yf_price = yf_price.loc['2020-07-01':'2020-11-30']\n",
    "yf_returns_monthly = yf_returns_monthly.loc['2020-07-01':'2020-11-30']\n",
    "yf_price_monthly = yf_price_monthly.loc['2020-07-01':'2020-11-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af46e1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Filling the index\n",
    "idx = daily_prices.index.union(yf_returns.index)\n",
    "daily_returns = daily_returns.reindex(idx)\n",
    "daily_prices = daily_prices.reindex(idx)\n",
    "daily_shares = daily_shares.reindex(idx)\n",
    "\n",
    "### Filling the data\n",
    "daily_returns.loc['2020-07-01':'2020-11-30'] = yf_returns\n",
    "daily_prices.loc['2020-07-01':'2020-11-30'] = yf_price\n",
    "\n",
    "### Filling the index\n",
    "idx = monthly_prices.index.union(yf_returns_monthly.index)\n",
    "monthly_returns = monthly_returns.reindex(idx)\n",
    "monthly_prices = monthly_prices.reindex(idx)\n",
    "monthly_shares = monthly_shares.reindex(idx)\n",
    "\n",
    "### Filling the data\n",
    "monthly_returns.loc['2020-07-01':'2020-11-30'] = yf_returns_monthly\n",
    "monthly_prices.loc['2020-07-01':'2020-11-30'] = yf_price_monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a4bac",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Fixing number of share for the missing period.\n",
    "\n",
    "More spesificaly, if we have the the numbers of share for the first or the last period, we simply fill the missing period in with this number. If we have the numbers of share for the first AND the last period, we interpolate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e200ffb9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Getting the dates where we are missing data\n",
    "shares_monthly_2020 = monthly_shares.loc['2020-06-01':'2020-12-31']\n",
    "shares_daily_2020 = daily_shares.loc['2020-06-30':'2020-12-01']\n",
    "\n",
    "### Looping trough it\n",
    "for i in range(len(shares_monthly_2020.columns)):\n",
    "    \n",
    "    ### Monthly\n",
    "    if not np.isnan(shares_monthly_2020.iloc[6, i]) or not np.isnan(shares_monthly_2020.iloc[6, i]):\n",
    "        if not np.isnan(shares_monthly_2020.iloc[6, i]) and not np.isnan(shares_monthly_2020.iloc[0, i]):\n",
    "            shares_monthly_2020.iloc[:, i] = shares_monthly_2020.iloc[:, i].interpolate()\n",
    "        elif not np.isnan(shares_monthly_2020.iloc[6, i]):\n",
    "            shares_monthly_2020.iloc[1:6, i] = shares_monthly_2020.iloc[6, i]\n",
    "        elif not np.isnan(shares_monthly_2020.iloc[0, i]):\n",
    "            shares_monthly_2020.iloc[1:6, i] = shares_monthly_2020.iloc[0, i]\n",
    "\n",
    "    ### Daily\n",
    "    if not np.isnan(shares_daily_2020.iloc[110, i]) or not np.isnan(shares_daily_2020.iloc[110, i]):\n",
    "        if not np.isnan(shares_daily_2020.iloc[110, i]) and not np.isnan(shares_daily_2020.iloc[0, i]):\n",
    "            shares_daily_2020.iloc[:, i] = shares_daily_2020.iloc[:, i].interpolate()\n",
    "        elif not np.isnan(shares_daily_2020.iloc[110, i]):\n",
    "            shares_daily_2020.iloc[1:110, i] = shares_daily_2020.iloc[110, i]\n",
    "        elif not np.isnan(shares_daily_2020.iloc[0, i]):\n",
    "            shares_daily_2020.iloc[1:110, i] = shares_daily_2020.iloc[0, i]\n",
    "\n",
    "### Filling the data\n",
    "monthly_shares.loc['2020-06-01':'2020-12-31'] = shares_monthly_2020\n",
    "daily_shares.loc['2020-06-30':'2020-12-01'] = shares_daily_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db4e7f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Getting the market cap\n",
    "monthly_mcap = monthly_prices * monthly_shares\n",
    "daily_mcap = daily_prices * daily_shares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5def4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Using the downloaded prices to compute all the numbers we can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649fcfb8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @njit\n",
    "# def fill_missing(prices, returns, mcap, shares):\n",
    "#     n_rows, n_cols = prices.shape\n",
    "#     for col in range(0, n_cols):\n",
    "#         for row in range(1, n_rows):\n",
    "#             # Fill missing price using previous price and current return\n",
    "#             if np.isnan(prices[row, col]):\n",
    "#                 if not np.isnan(prices[row - 1, col]) and not np.isnan(returns[row, col]):\n",
    "#                     prices[row, col] = prices[row - 1, col] * (1 + returns[row, col])\n",
    "#             # Fill missing return using current and previous price\n",
    "#             #if np.isnan(returns[row, col]):\n",
    "#             #    if not np.isnan(prices[row - 1, col]) and not np.isnan(prices[row, col]):\n",
    "#             #        returns[row, col] = (prices[row, col] / prices[row - 1, col]) - 1\n",
    "#             # Fill missing market cap using price and shares\n",
    "#             if np.isnan(mcap[row, col]):\n",
    "#                 if not np.isnan(prices[row, col]) and not np.isnan(shares[row, col]):\n",
    "#                     mcap[row, col] = prices[row, col] * shares[row, col]\n",
    "#     return prices, returns, mcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe285e6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Filling the missing values\n",
    "# m1, m2, m3 = fill_missing(monthly_prices.values, monthly_returns.values, monthly_mcap.values, monthly_shares.values)\n",
    "# d1, d2, d3 = fill_missing(daily_prices.values, daily_returns.values, daily_mcap.values, daily_shares.values)\n",
    "\n",
    "# ### Turning the arrays back into dataframes\n",
    "# daily_prices = pd.DataFrame(d1, columns=daily_prices.columns, index=daily_prices.index)\n",
    "# daily_returns = pd.DataFrame(d2, columns=daily_returns.columns, index=daily_returns.index)\n",
    "# daily_mcap = pd.DataFrame(d3, columns=daily_mcap.columns, index=daily_mcap.index)\n",
    "\n",
    "# ### Turning the arrays back into dataframes\n",
    "# monthly_prices = pd.DataFrame(m1, columns=monthly_prices.columns, index=monthly_prices.index)\n",
    "# monthly_returns = pd.DataFrame(m2, columns=monthly_returns.columns, index=monthly_returns.index)\n",
    "# monthly_mcap = pd.DataFrame(m3, columns=monthly_mcap.columns, index=monthly_mcap.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f816abe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Removing all columns that are only NaN or are dtypes object\n",
    "monthly_returns = monthly_returns.dropna(axis=1, how='all')\n",
    "monthly_returns = monthly_returns.select_dtypes(exclude=['object'])\n",
    "monthly_prices = monthly_prices[monthly_returns.columns]\n",
    "monthly_mcap = monthly_mcap[monthly_returns.columns]\n",
    "monthly_shares = monthly_shares[monthly_returns.columns]\n",
    "\n",
    "daily_returns = daily_returns[monthly_returns.columns]\n",
    "daily_prices = daily_prices[monthly_returns.columns]\n",
    "daily_mcap = daily_mcap[monthly_returns.columns]\n",
    "daily_shares = daily_shares[monthly_returns.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891f6a3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data manipulation done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d217e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Connecting to the database\n",
    "conn = sqlite3.connect('Data/data.db')\n",
    "\n",
    "### Saving the unfiltred data in the database\n",
    "daily_returns.to_sql('daily_returns', conn, if_exists='replace')\n",
    "daily_prices.to_sql('daily_prices', conn, if_exists='replace')\n",
    "daily_mcap.to_sql('daily_mcap', conn, if_exists='replace')\n",
    "daily_shares.to_sql('daily_shares', conn, if_exists='replace')\n",
    "\n",
    "monthly_returns.to_sql('monthly_returns', conn, if_exists='replace')\n",
    "monthly_prices.to_sql('monthly_prices', conn, if_exists='replace')\n",
    "monthly_mcap.to_sql('monthly_mcap', conn, if_exists='replace')\n",
    "monthly_shares.to_sql('monthly_shares', conn, if_exists='replace')\n",
    "\n",
    "### Closing the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc821aa9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Filtering\n",
    "\n",
    "- Removing all stock returns that trade at a price under 10 NOK that day/month. (If the market cap is missing or under 5 million)\n",
    "\n",
    "- Removing all stock returns that trade at a marketcap under 10 million NOK that day/month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a77af5f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def filter_returns(ret, mcap, price, mcapthreshold1=1000000, pricethreshold2=10):\n",
    "    rows, cols = ret.shape\n",
    "    for col in range(cols):\n",
    "        for row in range(rows):\n",
    "            if mcap[row, col] < mcapthreshold1:\n",
    "                ret[row, col] = np.nan\n",
    "                mcap[row, col] = np.nan\n",
    "                price[row, col] = np.nan\n",
    "            if mcap[row, col] == np.nan or mcap[row, col] < 10000000:\n",
    "                if price[row, col] < pricethreshold2:\n",
    "                    ret[row, col] = np.nan\n",
    "                    mcap[row, col] = np.nan\n",
    "                    price[row, col] = np.nan\n",
    "    return ret, mcap, price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff37acda",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m1, m2, m3 = filter_returns(monthly_returns.values, monthly_mcap.values, monthly_prices.values)\n",
    "d1, d2, d3 = filter_returns(daily_returns.values, daily_mcap.values, daily_prices.values)\n",
    "\n",
    "### Turning the arrays back into dataframes\n",
    "filtered_daily_returns = pd.DataFrame(d1, columns=daily_returns.columns, index=daily_returns.index)\n",
    "filtered_daily_prices = pd.DataFrame(d3, columns=daily_prices.columns, index=daily_prices.index)\n",
    "filtered_daily_mcap = pd.DataFrame(d2, columns=daily_mcap.columns, index=daily_mcap.index)\n",
    "\n",
    "### Turning the arrays back into dataframes\n",
    "filtered_monthly_returns = pd.DataFrame(m1, columns=monthly_returns.columns, index=monthly_returns.index)\n",
    "filtered_monthly_prices = pd.DataFrame(m3, columns=monthly_prices.columns, index=monthly_prices.index)\n",
    "filtered_monthly_mcap = pd.DataFrame(m2, columns=monthly_mcap.columns, index=monthly_mcap.index)\n",
    "\n",
    "### If we dont have 4 months of data, we remove the company\n",
    "filtered_monthly_returns = filtered_monthly_returns.dropna(axis=1, thresh=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8e60c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"We have {monthly_returns.shape[1]} tickers in the unfiltered monthly dataset\")\n",
    "print(f\"We have {filtered_daily_returns.shape[1]} tickers in the unfiltered daily dataset\")\n",
    "\n",
    "### Dropping the columns that are only NaN\n",
    "filtered_monthly_returns = filtered_monthly_returns.dropna(axis=1, how='all')\n",
    "filtered_daily_returns = filtered_daily_returns.dropna(axis=1, how='all')\n",
    "\n",
    "print(f\"We have {filtered_monthly_returns.shape[1]} tickers in the filtered monthly dataset\")\n",
    "print(f\"We have {filtered_daily_returns.shape[1]} tickers in the filtered daily dataset\")\n",
    "print(f\"We will only use the tickers that are in both filtered datasets, so we have {len(filtered_monthly_returns.columns)} tickers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733dd523",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### We will only use the stocks we have monthly returns for\n",
    "filtered_daily_prices = filtered_daily_prices[filtered_monthly_returns.columns]\n",
    "filtered_daily_mcap = filtered_daily_mcap[filtered_monthly_returns.columns]\n",
    "filtered_daily_shares = daily_shares[filtered_monthly_returns.columns]\n",
    "filtered_daily_returns = filtered_daily_returns[filtered_monthly_returns.columns]\n",
    "filtered_monthly_prices = filtered_monthly_prices[filtered_monthly_returns.columns]\n",
    "filtered_monthly_mcap = filtered_monthly_mcap[filtered_monthly_returns.columns]\n",
    "filtered_monthly_shares = monthly_shares[filtered_monthly_returns.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353072d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Connecting to the database\n",
    "conn = sqlite3.connect('Data/data.db')\n",
    "\n",
    "### Saving the filtred data in the database\n",
    "filtered_daily_returns.to_sql('filtered_daily_returns', conn, if_exists='replace')\n",
    "filtered_daily_prices.to_sql('filtered_daily_prices', conn, if_exists='replace')\n",
    "filtered_daily_mcap.to_sql('filtered_daily_mcap', conn, if_exists='replace')\n",
    "filtered_daily_shares.to_sql('filtered_daily_shares', conn, if_exists='replace')\n",
    "\n",
    "filtered_monthly_returns.to_sql('filtered_monthly_returns', conn, if_exists='replace')\n",
    "filtered_monthly_prices.to_sql('filtered_monthly_prices', conn, if_exists='replace')\n",
    "filtered_monthly_mcap.to_sql('filtered_monthly_mcap', conn, if_exists='replace')\n",
    "filtered_monthly_shares.to_sql('filtered_monthly_shares', conn, if_exists='replace')\n",
    "\n",
    "### Closing the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26bee8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### End timer\n",
    "end = time.time()\n",
    "\n",
    "print(f\"The script took {end - start} seconds to run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.755031,
   "end_time": "2025-02-17T11:05:43.780311",
   "environment_variables": {},
   "exception": null,
   "input_path": "00_data_stock_returns.ipynb",
   "output_path": "00_data_stock_returns.ipynb",
   "parameters": {},
   "start_time": "2025-02-17T11:05:12.025280",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}